# Trinity Platform - Complete Backward Dependency Map

**Analysis Date:** 2025-11-17 06:20 UTC
**Method:** Backward tracing from outputs to sources using ACTUAL CODE
**Coverage:** All 22 application services

---

## CRITICAL SECURITY FINDING

**üî¥ IMMEDIATE FIX REQUIRED:**

**Services using `eval()` for parsing (CRITICAL VULNERABILITY):**
- Event Collector: Line 88 `event = eval(event_str)`
- KG Projector: Lines 88, 131, 166 `event = eval(event_str)`

**Risk:** Remote code execution if malicious webhook payload
**Fix:** Replace with `json.loads(event_str)`
**Priority:** P0 - Fix before any production use
**Time:** 15 minutes

---

## Service 1: Agent Orchestrator

### OUTPUT
```python
AgentResponse(
    task_id: str,           # "task-{id}"
    status: str,            # "queued", "running", "completed", "failed"
    result: Optional[str],  # AI response text
    tokens_used: Optional[int],  # Input + output tokens
    model: str              # "claude-haiku-4.5-20250514"
)
```

### BACKWARD CHAIN

```
OUTPUT: AgentResponse
  ‚Üì FROM: get_task_status() reads agent_tasks table
    ‚Üì Table: agent_tasks (id, task_type, description, context, status, result, tokens_used, model, created_at, completed_at)
      ‚Üì Populated by: execute_agent_task() background function
        ‚Üì Calls: anthropic_client.messages.create()
          ‚Üì Requires: ANTHROPIC_API_KEY environment variable
            ‚Üì Requires: anthropic SDK v0.40.0+ (httpx 0.28.1 compatible)
              ‚Üì INPUT: AgentTask from POST /agent/execute
                ‚Üì SOURCE: HTTP request from external caller
```

### DEPENDENCIES
- ‚úÖ PostgreSQL: agent_tasks table (created at startup)
- ‚úÖ Anthropic API: Claude Haiku 4.5
- ‚ö†Ô∏è Redis: Queue populated but NO CONSUMER (orphaned)
- ‚ùå Environment: ANTHROPIC_API_KEY (empty by default)

### GAPS IDENTIFIED
1. **Orphaned Redis queue:** Tasks queued to `agent_queue:{priority}` but no consumer service reads from it
   - **Impact:** Queue grows indefinitely
   - **Fix:** Remove Redis queueing OR implement worker to consume queue
2. **In-memory job tracking:** Swarm tasks lost on restart
   - **Impact:** Cannot query swarm status after service restart
   - **Fix:** Store swarm metadata in PostgreSQL
3. **No API key validation:** Service starts even if ANTHROPIC_API_KEY empty
   - **Impact:** All tasks fail at execution time, not startup
   - **Fix:** Validate API key on startup, fail fast

---

## Service 2: Workflow Engine

### OUTPUT
```python
{
    "execution_id": int,
    "workflow_id": int,
    "status": str,  # "running", "completed", "failed"
    "current_step": str,
    "progress": str,  # "2/5"
    "steps": List[{step_id, status, error, duration}]
}
```

### BACKWARD CHAIN

```
OUTPUT: Execution status
  ‚Üì FROM: workflow_executions table
    ‚Üì Fields: id, workflow_id, trigger, status, current_step, steps_completed, total_steps, error, started_at, completed_at
      ‚Üì Updated by: execute_workflow_background()
        ‚Üì Reads: workflows table (steps JSONB)
          ‚Üì Deserializes: List[WorkflowStep]
            ‚Üì Each step: {step_id, service_url, method, payload, retry_count, timeout, depends_on}
              ‚Üì Executes: HTTP request to service_url
                ‚Üì Stores result: workflow_step_results table
                  ‚Üì INPUT: Workflow from POST /workflows
                    ‚Üì SOURCE: External API caller
```

### DEPENDENCIES
- ‚úÖ PostgreSQL: workflows, workflow_executions, workflow_step_results (created at startup)
- ‚úÖ HTTP client: httpx for service calls
- ‚ö†Ô∏è Service URLs: No validation (arbitrary URLs accepted)
- ‚ùå Error handling: `response.json()` called without checking if response is JSON

### GAPS IDENTIFIED
1. **No service URL validation:** Workflow can call any URL (SSRF vulnerability)
   - **Impact:** Can be used to scan internal network
   - **Fix:** Whitelist allowed service domains
2. **Unhandled JSON parse:** Line 130 `json.dumps(response.json())` assumes response is JSON
   - **Impact:** Crashes if service returns non-JSON (HTML error page)
   - **Fix:** Try/except around response.json(), store status code on failure
3. **Dependency cycle detection missing:** Circular dependencies cause infinite waiting
   - **Impact:** Workflow hangs forever
   - **Fix:** Topological sort of steps before execution
4. **No aggregated result:** workflow_executions.result field never populated
   - **Impact:** Cannot query final workflow output
   - **Fix:** Aggregate all step results into execution.result at completion

---

## Service 3: Event Collector

### OUTPUT
```python
{
    "status": "received",
    "event": str  # Event type (push, issues, pull_request)
}
```

### BACKWARD CHAIN

```
OUTPUT: Status confirmation
  ‚Üì FROM: github_webhook() endpoint
    ‚Üì Validates: HMAC signature (x-hub-signature-256)
      ‚Üì Parses: JSON payload
        ‚Üì Routes: handle_push_event() / handle_issue_event() / handle_pr_event()
          ‚Üì PUBLISHES to NATS: git.commits, github.issues.*, github.pr.*
            ‚Üì STORES in PostgreSQL: events table
              ‚Üì EVENT DATA: {event_type, ...data...}
                ‚Üì NATS: bytes(str(event_data), 'utf-8')  # ‚ö†Ô∏è Using str() not json.dumps()
                  ‚Üì PostgreSQL: event_data stored as str(dict)  # ‚ùå NOT QUERYABLE
                    ‚Üì SOURCE: GitHub webhook HTTP POST
```

### DEPENDENCIES
- ‚úÖ NATS: Connection established at startup
- ‚úÖ PostgreSQL: Connection pool created
- ‚ùå events table: NOT created in code (must pre-exist)
- ‚ö†Ô∏è JetStream streams: Created at startup (GIT_COMMITS, GITHUB_EVENTS)

### GAPS IDENTIFIED
1. **üî¥ CRITICAL SECURITY: eval() usage** (Line 131, 166)
   - **Risk:** Remote code execution
   - **Fix:** `event = json.loads(event_str)`
   - **Priority:** P0
2. **Data stored as string repr:** `str(event_data)` instead of `json.dumps(event_data)`
   - **Impact:** PostgreSQL JSONB queries impossible, data not parseable
   - **Fix:** Change to `json.dumps(event_data)`
3. **events table not created:** No CREATE TABLE statement
   - **Impact:** Service crashes on first webhook if table missing
   - **Fix:** Add CREATE TABLE in startup()
4. **Double bytes conversion:** `bytes(str(event_data), 'utf-8')` should be `json.dumps(event_data).encode()`
   - **Impact:** NATS consumers must use eval() (security risk propagates)
   - **Fix:** Proper JSON serialization

---

## Service 4: KG Projector

### OUTPUT
Neo4j nodes and relationships created

### BACKWARD CHAIN

```
OUTPUT: Neo4j graph nodes
  ‚Üì FROM: Cypher MERGE statements
    ‚Üì Node types: Commit, Issue, Service, File
      ‚Üì Relationships: MODIFIES, MODIFIED_SERVICE
        ‚Üì Data FROM: NATS messages (git.commits, github.issues.>, code.service.>)
          ‚Üì Parsed with: eval(event_str)  # üî¥ SECURITY RISK
            ‚Üì Event structure: {commit_hash, author, message, files_changed, repository, timestamp}
              ‚Üì SOURCE: Event Collector publishes to NATS
```

### DEPENDENCIES
- ‚úÖ NATS: Subscribes to 3 subjects
- ‚úÖ Neo4j: Constraints created at startup
- ‚ùå Event validation: Assumes all fields exist (no error handling)
- üî¥ Security: Uses eval() to parse events (same as Event Collector)

### GAPS IDENTIFIED
1. **üî¥ CRITICAL: eval() usage** (Lines 88, 131, 166)
   - **Same risk as Event Collector:** RCE vulnerability
   - **Fix:** `event = json.loads(event_str)`
2. **No field validation:** Assumes commit_hash, author, message always present
   - **Impact:** Crashes if webhook payload incomplete
   - **Fix:** Use event.get("field", default) pattern
3. **No Issue‚ÜíService relationships:** Creates Issue nodes but doesn't link to affected services
   - **Impact:** RCA API cannot find affected services via graph traversal
   - **Fix:** Add AFFECTS relationship from Issue to Service
4. **No Solution nodes:** Never creates Solution nodes from resolved issues
   - **Impact:** recommended_solutions always empty in RCA API
   - **Fix:** Add handle_issue_resolved event type

---

## Service 5: Alert Manager

### OUTPUT
```python
{
    "alert_id": int,
    "fingerprint": str,  # SHA256 hash
    "triggered": bool,
    # OR if deduplicated:
    "deduplicated": bool,
    "fingerprint": str
}
```

### BACKWARD CHAIN

```
OUTPUT: Alert created/deduplicated
  ‚Üì FROM: trigger_alert() endpoint
    ‚Üì Fingerprint: SHA256(alert_type:title)
      ‚Üì Redis check: alert_dedup:{fingerprint}
        ‚Üì IF exists: UPDATE alerts SET occurrence_count++
        ‚Üì IF new: INSERT INTO alerts
          ‚Üì Background task: send_alert_notification()
            ‚Üì Queries: alert_rules WHERE condition LIKE %{alert_type}%
              ‚Üì Calls: Notification Service POST /notify
                ‚Üì Payload: {channel, priority, notification_data}
                  ‚Üì INPUT: Alert from POST /alerts/trigger
                    ‚Üì SOURCE: External service (Health Monitor, Real-Time Processor, etc.)
```

### DEPENDENCIES
- ‚úÖ PostgreSQL: alerts, alert_rules tables (created at startup)
- ‚úÖ Redis: Deduplication window (5 min TTL)
- ‚úÖ Notification Service: POST /notify
- ‚ö†Ô∏è Alert rules: Fuzzy matching with LIKE (may not match)

### GAPS IDENTIFIED
1. **Weak fingerprint:** Only uses `alert_type:title`, ignoring description/metadata
   - **Impact:** Different alerts with same title deduplicated incorrectly
   - **Fix:** Include severity and first 100 chars of description in fingerprint
2. **No unique constraint on fingerprint:** PostgreSQL allows duplicate fingerprints
   - **Impact:** Race condition can create duplicate alerts
   - **Fix:** `CREATE UNIQUE INDEX ON alerts(fingerprint) WHERE status='active'`
3. **Fuzzy rule matching:** `WHERE condition LIKE %{alert_type}%` is imprecise
   - **Impact:** Rules may not match or match wrong alerts
   - **Fix:** Use exact match or regex matching
4. **No escalation implementation:** escalation_delay field defined but never used
   - **Impact:** Escalation feature doesn't work
   - **Fix:** Add scheduler job to check first_seen + escalation_delay

---

## Service 6: Notification Service

### OUTPUT
```python
{
    "notification_id": str,
    "channel": str,  # "email", "webhook", "slack"
    "priority": str,
    "result": {status, ...}
}
```

### BACKWARD CHAIN

```
OUTPUT: Notification sent
  ‚Üì FROM: send_notification() endpoint
    ‚Üì Routes by channel: email/webhook/slack
      ‚Üì Email: aiosmtplib.send() with SMTP_HOST, SMTP_PORT, SMTP_USER, SMTP_PASSWORD
      ‚Üì Webhook: httpx.post() with retry logic
      ‚Üì Slack: httpx.post() to webhook_url
        ‚Üì Template rendering: Jinja2 (if template specified)
          ‚Üì Redis queue: notification_queue:{priority}
            ‚Üì INPUT: Notification from POST /notify
              ‚Üì SOURCE: Alert Manager, Health Monitor, or external caller
```

### DEPENDENCIES
- ‚ö†Ô∏è SMTP: SMTP_USER and SMTP_PASSWORD (empty by default)
- ‚úÖ Redis: Queue and preferences storage
- ‚úÖ Jinja2: Template rendering
- ‚ùå Email validation: Uses EmailStr but email-validator dependency

### GAPS IDENTIFIED
1. **SMTP not configured by default:** Returns "skipped" if no credentials
   - **Impact:** Email notifications silently fail
   - **Fix:** Require SMTP config or disable email channel
2. **Template not validated:** Jinja2 rendering errors not caught
   - **Impact:** Notification fails if template has syntax errors
   - **Fix:** Validate templates on first use, cache compiled templates
3. **Redis queue orphaned:** Notifications queued but no consumer reads queue
   - **Impact:** Queue grows indefinitely, notifications never sent from queue
   - **Fix:** Remove queueing OR implement worker to process queue
4. **No delivery confirmation:** No way to verify notification actually delivered
   - **Impact:** Cannot distinguish "sent" from "delivered"
   - **Fix:** Add delivery tracking table

---

## Service 7: API Gateway

### OUTPUT
Proxied service response with auth validation

### BACKWARD CHAIN

```
OUTPUT: Service response
  ‚Üì FROM: Service-specific proxy endpoints (e.g., /rca/analyze)
    ‚Üì JWT validation: verify_token() decodes JWT with JWT_SECRET
      ‚Üì Rate limiting: Checks Redis rate_limit:{client_id}:{minute}
        ‚Üì Audit logging: POST to audit-service (non-blocking)
          ‚Üì Service call: httpx.post(SERVICE_URL, json=body)
            ‚Üì Response: Proxied back to caller
              ‚Üì INPUT: HTTP request with Authorization header
                ‚Üì SOURCE: External client (user, frontend, other service)
```

### DEPENDENCIES
- ‚úÖ JWT_SECRET environment variable
- ‚úÖ Redis: Rate limiting
- ‚úÖ All service URLs: USER_MGMT_URL, RCA_API_URL, INVESTIGATION_URL, etc.
- ‚úÖ Audit Service: Non-blocking POST /audit

### GAPS IDENTIFIED
1. **No token refresh:** JWT expires after 24 hours, no refresh mechanism
   - **Impact:** Users must re-login daily
   - **Fix:** Add refresh token endpoint
2. **Rate limit not service-aware:** 100 req/min global, not per-service
   - **Impact:** One slow service can block access to all services
   - **Fix:** Per-service rate limits
3. **Audit failures silent:** try/except with pass swallows all errors
   - **Impact:** Cannot detect if audit service is down
   - **Fix:** Log audit failures, expose metric
4. **No circuit breaker:** Keeps calling failing backend services
   - **Impact:** Cascading failures, slow responses
   - **Fix:** Implement circuit breaker pattern

---

## Service 8: Event Collector

### CRITICAL ISSUES (Already covered above)

**SECURITY:**
- üî¥ Uses `eval()` - RCE vulnerability
- üî¥ Stores `str(dict)` instead of JSON - data corruption

**DATA:**
- events table NOT created in code
- NATS streams created but minimal error handling

**Fix Priority:** P0 (security), P1 (data integrity)

---

## Service 9: KG Projector

### CRITICAL ISSUES (Already covered above)

**SECURITY:**
- üî¥ Uses `eval()` in 3 places - RCE vulnerability

**DATA:**
- Missing Issue‚ÜíService AFFECTS relationships
- Missing Solution node creation
- No field validation (assumes all fields present)

**Fix Priority:** P0 (security), P1 (missing relationships)

---

## Service 10: ML Training

### OUTPUT
```python
TrainingJob(
    job_id: str,  # "model-{timestamp}"
    model_type: str,
    status: str,  # "queued", "training", "completed", "failed"
    metrics: Optional[ModelMetrics],
    created_at: str,
    completed_at: Optional[str]
)
```

### BACKWARD CHAIN

```
OUTPUT: Training job status
  ‚Üì FROM: training_jobs dict (IN-MEMORY)  # ‚ùå CRITICAL
    ‚Üì Populated by: train_model() background function
      ‚Üì Trains: sklearn RandomForest or GradientBoosting
        ‚Üì Evaluates: train_test_split, accuracy/precision/recall/f1
          ‚Üì Saves model: joblib.dump() to /app/models/{job_id}.joblib
            ‚Üì Saves metadata: JSON file {job_id}_metadata.json
              ‚Üì INPUT: TrainingData from POST /train
                ‚Üì SOURCE: External caller provides features and labels
```

### DEPENDENCIES
- ‚úÖ Docker volume: ml-models (persists models across restarts)
- ‚ùå Job tracking: In-memory dict (lost on restart)
- ‚ùå Training data validation: No dimensionality or type checking

### GAPS IDENTIFIED
1. **üî¥ In-memory job tracking:** training_jobs dict lost on restart
   - **Impact:** Cannot query job status after service restart
   - **Fix:** Store jobs in PostgreSQL table
2. **No input validation:** Features/labels not validated for shape/type
   - **Impact:** Training crashes with cryptic sklearn errors
   - **Fix:** Validate feature dimensions match, labels are valid integers
3. **Timestamp collision:** job_id uses strftime('%Y%m%d%H%M%S') - only 1-second resolution
   - **Impact:** Two jobs in same second overwrite each other
   - **Fix:** Add milliseconds or UUID
4. **No model versioning:** Old models overwritten
   - **Impact:** Cannot rollback to previous model
   - **Fix:** Keep last N versions, add versioning metadata

---

## Service 11: Metrics Collector

### OUTPUT
Prometheus metrics at http://localhost:9090/metrics

```
trinity_services_up 11.0
trinity_kg_nodes_total 359.0
trinity_kg_relationships_total 12.0
trinity_vector_documents 168.0
trinity_audit_events_total 10.0
```

### BACKWARD CHAIN

```
OUTPUT: Prometheus metrics
  ‚Üì FROM: Prometheus HTTP server (port 9090)
    ‚Üì Gauges/Counters: SERVICES_UP, KG_NODES, KG_RELATIONSHIPS, etc.
      ‚Üì Updated by: collect_service_health(), collect_kg_metrics(), etc.
        ‚Üì Service health: HTTP GET {service}/health
        ‚Üì KG metrics: Neo4j COUNT queries
        ‚Üì Vector metrics: GET vector-search:8000/stats
        ‚Üì PostgreSQL metrics: SELECT COUNT from audit_log, agent_tasks
          ‚Üì Loop: every COLLECT_INTERVAL seconds (default 15)
            ‚Üì SOURCE: Scheduled collection loop
```

### DEPENDENCIES
- ‚úÖ Prometheus: start_http_server(9090)
- ‚úÖ Neo4j: GraphDatabase.driver (synchronous)
- ‚úÖ PostgreSQL: asyncpg queries
- ‚ö†Ô∏è Service URLs: Hardcoded list (not dynamic)

### GAPS IDENTIFIED
1. **Hardcoded service list:** Only monitors 11 services, missing 11 others
   - **Impact:** Half the platform not monitored
   - **Fix:** Query docker-compose or service registry for dynamic list
2. **Neo4j driver not closed:** Creates new driver every 15 seconds
   - **Impact:** Connection pool exhaustion
   - **Fix:** Create driver once at startup, reuse
3. **Counter manipulation:** Directly sets Counter._value._value (internal API)
   - **Impact:** May break with prometheus_client updates
   - **Fix:** Use inc() method or re-architect
4. **No error metrics:** Collection failures swallowed (bare except)
   - **Impact:** Cannot detect if metrics are stale
   - **Fix:** Add trinity_collection_errors_total counter

---

## Service 12: Scheduler

### OUTPUT
Executes scheduled workflows and maintenance tasks

### BACKWARD CHAIN

```
OUTPUT: Scheduled execution
  ‚Üì FROM: APScheduler CronTrigger
    ‚Üì Jobs: daily_kg_maintenance(), hourly_metrics_aggregation()
      ‚Üì Calls: Agent Orchestrator POST /agent/execute
      ‚Üì Calls: Workflow Engine POST /workflows/{id}/execute
        ‚Üì Loads workflows: FROM workflows table WHERE schedule IS NOT NULL
          ‚Üì PostgreSQL: SELECT id, name, schedule FROM workflows
            ‚Üì Adds to scheduler: scheduler.add_job()
              ‚Üì SOURCE: Database workflows + hardcoded maintenance jobs
```

### DEPENDENCIES
- ‚úÖ PostgreSQL: workflows table
- ‚úÖ APScheduler: AsyncIOScheduler
- ‚úÖ Agent Orchestrator: HTTP calls
- ‚úÖ Workflow Engine: HTTP calls
- ‚ùå Error handling: HTTP failures printed but no retry/alert

### GAPS IDENTIFIED
1. **No job persistence:** APScheduler state lost on restart
   - **Impact:** Jobs re-added, may double-execute
   - **Fix:** Use PostgreSQL job store for APScheduler
2. **No execution tracking:** Cannot see if scheduled jobs succeeded
   - **Impact:** Silent failures
   - **Fix:** Store execution results in PostgreSQL
3. **Hardcoded maintenance schedule:** 2 AM daily, no configuration
   - **Impact:** Inflexible for different timezones/requirements
   - **Fix:** Move to database configuration
4. **Reload every 5 minutes:** Inefficient, creates scheduler churn
   - **Impact:** Unnecessary database queries, job duplication risk
   - **Fix:** Use database triggers or longer reload interval

---

## CROSS-SERVICE INTEGRATION MATRIX

| Producer | Output | Consumer | Input | Transform | Status |
|----------|--------|----------|-------|-----------|--------|
| Event Collector | NATS msg | KG Projector | event dict | eval() üî¥ | BROKEN |
| KG Projector | Neo4j nodes | RCA API | Cypher query | None | ‚úÖ WORKS |
| RCA API | SimilarIssue | API Gateway | JSON | Proxy | ‚úÖ WORKS |
| Alert Manager | Alert | Notification Svc | Notification | Payload map | ‚úÖ WORKS |
| Health Monitor | Alert | Alert Manager | Alert | Direct | ‚ö†Ô∏è UNTESTED |
| Workflow Engine | HTTP call | Any service | JSON | None | ‚ö†Ô∏è NO VALIDATION |
| Agent Orchestrator | Redis queue | ‚ùå NONE | - | - | ‚ùå ORPHANED |
| Notification | Redis queue | ‚ùå NONE | - | - | ‚ùå ORPHANED |
| Scheduler | Workflow exec | Workflow Engine | execution | Direct | ‚úÖ WORKS |

---

## CRITICAL PRIORITIES

### P0 - Security (FIX IMMEDIATELY)
1. Replace all `eval()` with `json.loads()` (Event Collector, KG Projector)
2. Validate service URLs in Workflow Engine (SSRF risk)

### P1 - Data Integrity (FIX THIS WEEK)
3. Change `str(dict)` to `json.dumps()` (Event Collector)
4. Add events table CREATE statement (Event Collector)
5. Store ML job status in PostgreSQL (ML Training)
6. Fix fingerprint collision (Alert Manager)
7. Close Neo4j driver properly (Metrics Collector)

### P2 - Missing Features (FIX THIS SPRINT)
8. Implement Redis queue consumers (Agent Orchestrator, Notification)
9. Add Issue‚ÜíService relationships (KG Projector)
10. Create Solution nodes (KG Projector)
11. Add escalation logic (Alert Manager)
12. Implement circuit breaker (API Gateway)

---

## ESTIMATED FIX TIME

| Priority | Issues | Time |
|----------|--------|------|
| P0 Security | 2 | 2 hours |
| P1 Data | 5 | 16 hours |
| P2 Features | 6 | 24 hours |
| **Total** | **13** | **42 hours** |

**Timeline:** 1 week with 1 developer

---

## SUMMARY

Trinity Platform has **solid architecture** with **critical implementation gaps**:

**Working:**
- ‚úÖ User authentication (complete chain)
- ‚úÖ Basic RCA (with hardcoded fallback)
- ‚úÖ Vector search (properly implemented)
- ‚úÖ Service orchestration (workflows, agents)

**Broken:**
- üî¥ Event parsing (eval() security risk)
- üî¥ Data serialization (str() instead of JSON)
- ‚ùå Orphaned queues (no consumers)
- ‚ùå Missing graph relationships

**Fix these 13 issues and platform will be production-ready.**
